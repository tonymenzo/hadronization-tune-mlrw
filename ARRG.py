import numpy as np
from tqdm import tqdm

import torch
from torch import Tensor
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

import os

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
sns.set_style("ticks")
sns.set_context("paper", font_scale = 1.8)
plt.rcParams['text.usetex'] = True
plt.rcParams['font.size'] = 25

from lund_weight import LundWeight

class ARRG():
    def __init__(self, epochs, dim_multiplicity, dim_accept_reject, over_sample_factor, params_base,
                 sim_observable_dataloader, sim_kinematics_z_dataloader, sim_kinematics_mT_dataloader, 
                 exp_observable_dataloader, params_init = None, print_details = False, results_dir = None):
        """
        ARRG training class for tuning microscopic dynamics (hadronization parameters) from macroscopic observables.

        epochs (int): ---------------------- Number of training cycles through the full dataset
        dim_multiplicity (int): ------------ Size of the zero-padded hadronization data (largest fragmentation chain length)
        dim_accept_reject (int): ----------- Size of zero-padded accept-reject arrays (largest rejection count in events)
        over_sample_factor (float): --------
        params_base (torch tensor): -------- 
        sim_observable_dataloader (string): - Path to global observable data generated by sim model
        sim_kinematics_dataloader (string): - Path to sim model kinematics generating the global observable
        exp_observable_dataloader (string): - Path to desired (experimental) global distribution (training data)
        params_init (torch tensor): ---------
        print_details (bool): --------------- Option to output intermediate results during training
        results_dir (string): --------------- Option for path to store results (if the directory doesn't exist, it will be created)
        """

        # Model hyperparameters
        self.epochs = epochs
        self.dim_multiplicity = dim_multiplicity 
        self.dim_accept_reject = dim_accept_reject
        self.over_sample_factor = over_sample_factor
        self.params_base = params_base
        if params_init == None:
            self.params_init = self.params_base
        else:
            self.params_init = params_init

        # Training data
        self.exp_observable = exp_observable_dataloader
        self.sim_observable_base = sim_observable_dataloader
        self.sim_z_base = sim_kinematics_z_dataloader
        self.sim_mT_base = sim_kinematics_mT_dataloader
        self.print_details = print_details
        self.results_dir = results_dir

        # Define the weiLund weight layer
        self.weight_nexus = LundWeight(self.params_base, self.params_init, over_sample_factor = self.over_sample_factor)

        # Create a results directory if it doesn't exist
        if not os.path.exists(self.results_dir):
            os.mkdir(self.results_dir) 
            print('A model directory was created at,', self.results_dir)
    
    def histogram(self, observable, weights = None, bins=50, min=0.0, max=1.0):
        """
        Generate a differentiable weighted histogram.
        """
        n_samples, n_chns = 1, 1
        
        # Initialize bins
        hist_torch = torch.zeros(n_samples, n_chns, bins, device=observable.device)
        delta = (max - min) / bins
        bin_table = torch.linspace(min, max, steps=bins, device=observable.device)
    
        # Perform the binning
        for dim in range(1, bins - 1):
            h_r_sub_1, h_r, h_r_plus_1 = bin_table[dim - 1: dim + 2]
    
            mask_sub = ((h_r > observable) & (observable >= h_r_sub_1)).float()
            mask_plus = ((h_r_plus_1 > observable) & (observable >= h_r)).float()

            if weights == None:
                hist_torch[:, :, dim].add_(torch.sum(((observable - h_r_sub_1) * mask_sub).view(n_samples, n_chns, -1), dim=-1))
                hist_torch[:, :, dim].add_(torch.sum(((h_r_plus_1 - observable) * mask_plus).view(n_samples, n_chns, -1), dim=-1))
            else:
                hist_torch[:, :, dim].add_(torch.sum(((observable - h_r_sub_1) * mask_sub * weights).view(n_samples, n_chns, -1), dim=-1))
                hist_torch[:, :, dim].add_(torch.sum(((h_r_plus_1 - observable) * mask_plus * weights).view(n_samples, n_chns, -1), dim=-1))

        # Normalize the histogram so that the sum across all bins is 1
        hist_torch /= hist_torch.sum(dim=-1, keepdim=True)

        return (hist_torch / delta).squeeze(), bin_table

    def binned_loss(self, sim_observable, exp_observable, weights):
        """
        Loss function which creates a n-dimensional density estimation 
        and takes the mean-squared-error of an n-dimensional binning. 
        """
        # Find min and max of observables
        minimum = torch.min(torch.minimum(sim_observable[:,0], exp_observable[:,0]))
        maximum = torch.max(torch.maximum(sim_observable[:,0], exp_observable[:,0]))
        
        # Perform the binning of the macroscopic observables
        histo_sim, bins_sim = self.histogram(sim_observable[:,0].unsqueeze(0), weights = weights, bins = int(maximum - minimum), min = minimum, max = maximum)
        histo_exp, bins_exp = self.histogram(exp_observable[:,0].unsqueeze(0), bins = int(maximum - minimum), min = minimum, max = maximum)
        
        # Compute the psuedo-chi^2
        """
        # TBD insert stochastic uncertainty into the denominator of the loss with 1% lower bound as done in 1610.08328 
        error = torch.ones(len(bins_exp))
        error[(histo_exp > 0.) | (histo_sim > 0.)] = torch.div(error, (histo_exp + histo_sim))
        """
        pseudo_chi2 = (torch.pow((histo_sim - histo_exp), 2))

        if self.print_details:
            # Bin the simualted observable
            histo_sim_OG, bins_sim_OG = self.histogram(sim_observable[:,0].unsqueeze(0), bins = int(maximum - minimum), min = minimum, max = maximum)
            # Plot historgrams to ensure reweighting is working as expected
            fig, ax = plt.subplots(1,1,figsize = (6,5))
            ax.plot(bins_sim.detach().numpy(), histo_sim.detach().numpy(), '-o', label = r'$\mathrm{Weighted}$')
            ax.plot(bins_exp.detach().numpy(), histo_exp.detach().numpy(), '-o', label = r'$\mathrm{Exp.}$')
            ax.plot(bins_sim_OG.detach().numpy(), histo_sim_OG.detach().numpy(), '-o', label = r'$\mathrm{Sim.}$')
            ax.legend(frameon = False)
            fig.tight_layout()
            fig.savefig(self.results_dir + r'/loss_binning_check.pdf', dpi=300, pad_inches = .1, bbox_inches = 'tight')
            plt.close(fig)

        return torch.sum(pseudo_chi2)

    def train_ARRG(self, optimizer, scheduler=None):
        """
        Training cycle for ARRG. 

        optimizer: Specified network optimizer
        scheduler: Specified learning rate scheduler
        """
        a_b = [np.array([self.weight_nexus.params_a.clone().detach().numpy(), self.weight_nexus.params_b.clone().detach().numpy()])]
        #loss_epoch = []
        
        # TBD: Optimization for GPU training
        for i in tqdm(range(self.epochs), ncols = 100):
            device = "cpu"
            for (x,y,z,w) in zip(self.sim_z_base, self.sim_mT_base, self.sim_observable_base, self.exp_observable):
                x, y, z, w = x.to(device), y.to(device), z.to(device), w.to(device)
                # Reset the gradients in the optimizer
                optimizer.zero_grad()
                # Compute the weights
                weights = self.weight_nexus(x, y, z)
                # Compute the loss
                loss = self.binned_loss(z, w, weights)
                # Compute gradients
                loss.backward()
                # Update the network weights
                optimizer.step()
                # Update the learning rate scheduler
                scheduler.step(loss)
                
                """
                # Constrain weight layer parameters within allowed Pythia range
                switch = 0
                for p in self.weight_nexus.parameters():
                    if switch == 0:
                        switch += 1
                        p.data.clamp_(0.0, 2.0)
                    else:
                        p.data.clamp_(0.2, 2.0)
                """

                # Output the loss and learning rate 
                print(f'Loss: {loss.clone().detach().numpy():>8f}, LR: {optimizer.param_groups[0]["lr"]:>8f}')
                print(f'a: {self.weight_nexus.params_a.clone().detach().numpy()}, b: {self.weight_nexus.params_b.clone().detach().numpy()}')

                # Record the epoch loss
                #loss_epoch.append(loss.detach().numpy())

                # Record the tuned parameters
                a_b.append(np.array([self.weight_nexus.params_a.clone().detach().numpy(), self.weight_nexus.params_b.clone().detach().numpy()]))
                
                if self.print_details:
                    # Check the histograms
                    _, bins_exp = np.histogram(w[:,0].detach().numpy())
                    _, bins_sim = np.histogram(z[:,0].detach().numpy())
                    _, bins_fine_tuned = np.histogram(z[:,0].detach().numpy(), weights = weights.detach().numpy())
    
                    min_exp, max_exp = bins_exp[0], bins_exp[-1]
                    min_sim, max_sim = bins_sim[0], bins_sim[-1]
                    min_fine_tuned, max_fine_tuned = bins_fine_tuned[0], bins_fine_tuned[-1]
                    
                    # Plot multiplicity
                    fig_1, ax_1 = plt.subplots(1,1,figsize=(6,5))
                    ax_1.hist(z[:,0].detach().numpy(), int(max_sim - min_sim), label = r'$\mathrm{Base}$', alpha = 0.5, density = True, edgecolor = 'black')
                    ax_1.hist(w[:,0].detach().numpy(), int(max_exp - min_exp), label = r'$\mathrm{Exp.}$', alpha = 0.5, density = True, edgecolor = 'black')
                    ax_1.hist(z[:,0].detach().numpy(), int(max_fine_tuned - min_fine_tuned), weights = weights.detach().numpy(), label = r'$\mathrm{Tuned}$', alpha = 0.5, density = True, edgecolor = 'black')
                    ax_1.set_xlabel(r'$N_h$')
                    ax_1.set_ylabel(r'$\mathrm{Count}$')
                    ax_1.legend(frameon=False)
                    fig_1.tight_layout()
                    fig_1.savefig(self.results_dir + r'/ARRG_multiplicity_base_vs_exp_vs_tuned.pdf', dpi=300, pad_inches = .1, bbox_inches = 'tight')
    
                    # Plot sphericity
                    fig_2, ax_2 = plt.subplots(1,1,figsize=(6,5))
                    bins = np.histogram(np.hstack((z[:,1].detach().numpy(), w[:,1].detach().numpy())), bins=50)[1]
                    ax_2.hist(z[:,1].detach().numpy(), bins, label = r'$\mathrm{Base}$', alpha = 0.5, density = True, edgecolor = 'black')
                    ax_2.hist(w[:,1].detach().numpy(), bins, label = r'$\mathrm{Exp.}$', alpha = 0.5, density = True, edgecolor = 'black')
                    ax_2.hist(z[:,1].detach().numpy(), bins, weights = weights.detach().numpy(), label = r'$\mathrm{Tuned}$', alpha = 0.5, density = True, edgecolor = 'black')
                    ax_2.set_xlabel(r'$S$')
                    ax_2.set_ylabel(r'$\mathrm{Count}$')
                    ax_2.legend(frameon=False)
                    fig_2.tight_layout()
                    fig_2.savefig(self.results_dir + r'/ARRG_sphericity_base_vs_exp_vs_tuned.pdf', dpi=300, pad_inches = .1, bbox_inches = 'tight')
    
                    # Plot the search space
                    fig_3, ax_3 = plt.subplots(1,1,figsize=(6,5))
                    ax_3.plot(np.array(a_b)[:,0], np.array(a_b)[:,1], 'o-', ms = 1.5, alpha = 1.0, color = 'blue')
                    ax_3.plot(0.6, 1.5, 'x', color='green', label = r'$\mathrm{Target}$')
                    ax_3.plot(self.params_init[0].clone().detach().numpy(), self.params_init[1].clone().detach().numpy(), 'x', color = 'red', label = r'$\mathrm{Initial}$')
                    ax_3.axvline(0.6, ls = '--', color = 'green', alpha = 0.3)
                    ax_3.axvline(1.5, ymax = 0.75, ls = '--', color = 'red', alpha = 0.3)
                    ax_3.axhline(0.6, ls = '--', color = 'red', alpha = 0.3)
                    ax_3.axhline(1.5, xmax = 0.67, ls = '--', color = 'green', alpha = 0.3)
                    ax_3.set_xlim(0.5, 1.6)
                    ax_3.set_ylim(0.5, 1.6)
                    ax_3.set_xlabel(r'$a$')
                    ax_3.set_ylabel(r'$b$')
                    ax_3.legend(frameon = False, loc = 'upper right')
                    fig_3.tight_layout()
                    fig_3.savefig(self.results_dir + r'/ARRG_search_space.pdf', dpi=300, pad_inches = .1, bbox_inches = 'tight')
                    
                    # Close figures so RAM isn't soaked up
                    plt.close(fig_1)
                    plt.close(fig_2)
                    plt.close(fig_3)
                
        return np.array([self.weight_nexus.params_a.clone().detach().numpy(), self.weight_nexus.params_b.clone().detach().numpy()]), a_b