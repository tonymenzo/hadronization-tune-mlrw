{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ARRG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservableDataset(Dataset):\n",
    "\t\"\"\"\n",
    "\tConverts observable dataset into PyTorch syntax.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, data):\n",
    "\t\tself.data = data\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.data.shape[0]\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tsample = self.data[idx]\n",
    "\t\treturn sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do some data proccessing to extract the event-level observables we want to train with. Remember that the event level data-structure contains arrays of hadron-level kinematic data $p_x, p_y, p_z, E, m_h$. To need to compute event-level, experiment-level, or macroscopic-level observables that can actually be observed from experiment.\n",
    "\n",
    "I'll only use hadron multiplicity but we could also generate more distributions (sphericity, thrust, shape parameters, energy correlators, etc.) if desired. Hadron multiplicity is extremely sensitive to $a$ and $b$ so we can tune these parameters essentially exclusively on hadron multiplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental observable shape: (1000000, 50, 5)\n",
      "Simulated observable shape: (1000000, 50, 5)\n",
      "Simulated z shape: (1000000, 50, 100)\n",
      "Experimental multiplicity shape: torch.Size([100000])\n",
      "Simulated multiplicity shape: torch.Size([100000])\n",
      "Simulated z shape: torch.Size([100000, 50, 100])\n",
      "Simulated mT shape: torch.Size([100000, 50])\n"
     ]
    }
   ],
   "source": [
    "# Paths to the datasets on perlmutter\n",
    "exp_hadrons_PATH = '/global/cfs/projectdirs/m3246/hadronization-tune-mlrw-data/pgun_qqbar_hadrons_a_0.68_b_0.98_sigma_0.335_N_1e6.npy'\n",
    "#exp_accept_reject_PATH = '/global/cfs/projectdirs/m3246/hadronization-tune-mlrw-data/pgun_qqbar_accept_reject_z_a_0.68_b_0.98_sigma_0.335_N_1e6.npy'\n",
    "sim_hadrons_PATH = '/global/cfs/projectdirs/m3246/hadronization-tune-mlrw-data/pgun_qqbar_hadrons_a_0.72_b_0.88_sigma_0.335_N_1e6.npy'\n",
    "sim_accept_reject_PATH = '/global/cfs/projectdirs/m3246/hadronization-tune-mlrw-data/pgun_qqbar_accept_reject_z_a_0.72_b_0.88_sigma_0.335_N_1e6.npy'\n",
    "\n",
    "# Load the arrays\n",
    "exp_hadrons       = np.load(exp_hadrons_PATH, mmap_mode=\"r\")\n",
    "sim_hadrons       = np.load(sim_hadrons_PATH, mmap_mode=\"r\")\n",
    "sim_accept_reject = np.load(sim_accept_reject_PATH, mmap_mode = \"r\")\n",
    "\n",
    "# Print dataset shapes\n",
    "print('Experimental observable shape:', exp_hadrons.shape)\n",
    "print('Simulated observable shape:', sim_hadrons.shape)\n",
    "print('Simulated z shape:', sim_accept_reject.shape)\n",
    "\n",
    "# Restrict to a subset of the full dataset (for memory)\n",
    "N_events = int(100000)\n",
    "\n",
    "# Extract the hadron multiplicity\n",
    "exp_mult = np.array([len(exp_hadrons[i,:][np.abs(exp_hadrons[i,:,0]) > 0.0]) for i in range(exp_hadrons.shape[0])])\n",
    "sim_mult = np.array([len(sim_hadrons[i,:][np.abs(sim_hadrons[i,:,0]) > 0.0]) for i in range(sim_hadrons.shape[0])])\n",
    "# Extract the transverse mass\n",
    "sim_mT   = np.sqrt(sim_hadrons[:,:,0]**2 + sim_hadrons[:,:,1]**2 + sim_hadrons[:,:,4]**2)\n",
    "\n",
    "# Convert into torch objects\n",
    "sim_mult          = torch.Tensor(sim_mult[0:N_events].copy())\n",
    "sim_accept_reject = torch.Tensor(sim_accept_reject[0:N_events].copy())\n",
    "sim_mT            = torch.Tensor(sim_mT[0:N_events].copy())\n",
    "exp_mult          = torch.Tensor(exp_mult[0:N_events].copy())\n",
    "\n",
    "# Check the accepted z-values, if z == 1 reduce it by epsilon (a very nasty bug to find).\n",
    "# The a-coefficient when computing the likelihood has a term propotional to log(1-z). If \n",
    "# z = 1, this term diverges to -inf and completely destroys the backward pass.\n",
    "epsilon = 1e-5\n",
    "sim_accept_reject[sim_accept_reject == 1] = 1 - epsilon\n",
    "\n",
    "# Print dataset shapes\n",
    "print('Experimental multiplicity shape:', exp_mult.shape)\n",
    "print('Simulated multiplicity shape:', sim_mult.shape)\n",
    "print('Simulated z shape:', sim_accept_reject.shape)\n",
    "print('Simulated mT shape:', sim_mT.shape)\n",
    "\n",
    "# Prepare data for DataLoader\n",
    "sim_mult          = ObservableDataset(sim_mult)\n",
    "sim_accept_reject = ObservableDataset(sim_accept_reject)\n",
    "sim_mT            = ObservableDataset(sim_mT)\n",
    "exp_mult          = ObservableDataset(exp_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size -- TBD: Implement batch size scheduler\n",
    "batch_size = 5000\n",
    "\n",
    "# Initialize data-loaders\n",
    "sim_observable_dataloader    = DataLoader(sim_mult, batch_size = batch_size, shuffle = False)\n",
    "sim_accept_reject_dataloader = DataLoader(sim_accept_reject, batch_size = batch_size, shuffle = False)\n",
    "sim_mT_dataloader            = DataLoader(sim_mT, batch_size = batch_size, shuffle = False)\n",
    "exp_observable_dataloader    = DataLoader(exp_mult, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each event has been zero-padded to a length of 50\n",
      "Each emission has been zero-padded to a length of 100\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 2\n",
    "over_sample_factor = 10.0\n",
    "learning_rate = 0.01\n",
    "# Length of event buffer\n",
    "dim_multiplicity = sim_accept_reject_dataloader.dataset.data.shape[1]\n",
    "dim_accept_reject = sim_accept_reject_dataloader.dataset.data.shape[2]\n",
    "\n",
    "print('Each event has been zero-padded to a length of', dim_multiplicity)\n",
    "print('Each emission has been zero-padded to a length of', dim_accept_reject)\n",
    "\n",
    "# Define base parameters of simulated data (a, b)\n",
    "params_base = torch.tensor([0.72, 0.88])\n",
    "# If params_init is set equal to None, the tuned parameters are initialized to the base parameters\n",
    "params_init = None\n",
    "#params_init = torch.tensor([0.6, 1.5])\n",
    "\n",
    "print_details = True\n",
    "results_dir = r'./ARRG_a_b_tune_perlmutter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7200, 0.8800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 0\n",
      "----------------------------------------------\n",
      "Loss: 0.001539732\n",
      "Gradient of a: 0.019106671\n",
      "Gradient of b: -0.010020412\n",
      "Loss: 0.001540, LR: 0.010000\n",
      "a: 0.7100000381469727, b: 0.8899999856948853\n",
      "----------------------------------------------\n",
      "Batch # 1\n",
      "----------------------------------------------\n",
      "Loss: 0.002046078\n",
      "Gradient of a: 0.02132484\n",
      "Gradient of b: -0.011841806\n",
      "Loss: 0.002046, LR: 0.010000\n",
      "a: 0.6999865174293518, b: 0.9000087380409241\n",
      "----------------------------------------------\n",
      "Batch # 2\n",
      "----------------------------------------------\n",
      "Loss: 0.0009167767\n",
      "Gradient of a: 0.013805228\n",
      "Gradient of b: -0.008636051\n",
      "Loss: 0.000917, LR: 0.010000\n",
      "a: 0.6902390718460083, b: 0.9098764061927795\n",
      "----------------------------------------------\n",
      "Batch # 3\n",
      "----------------------------------------------\n",
      "Loss: 0.00086725253\n",
      "Gradient of a: 0.013185384\n",
      "Gradient of b: -0.0068583894\n",
      "Loss: 0.000867, LR: 0.010000\n",
      "a: 0.6806364059448242, b: 0.9195113182067871\n",
      "----------------------------------------------\n",
      "Batch # 4\n",
      "----------------------------------------------\n",
      "Loss: 0.00062584766\n",
      "Gradient of a: 0.011161224\n",
      "Gradient of b: -0.0061116507\n",
      "Loss: 0.000626, LR: 0.010000\n",
      "a: 0.6712318658828735, b: 0.9289339184761047\n",
      "----------------------------------------------\n",
      "Batch # 5\n",
      "----------------------------------------------\n",
      "Loss: 0.00055605645\n",
      "Gradient of a: 0.010813032\n",
      "Gradient of b: -0.0048749745\n",
      "Loss: 0.000556, LR: 0.010000\n",
      "a: 0.6619693040847778, b: 0.9380620718002319\n",
      "----------------------------------------------\n",
      "Batch # 6\n",
      "----------------------------------------------\n",
      "Loss: 0.00043410884\n",
      "Gradient of a: 0.0075870566\n",
      "Gradient of b: -0.0046426244\n",
      "Loss: 0.000434, LR: 0.010000\n",
      "a: 0.6530398726463318, b: 0.9469542503356934\n",
      "----------------------------------------------\n",
      "Batch # 7\n",
      "----------------------------------------------\n",
      "Loss: 0.00034072425\n",
      "Gradient of a: 0.005222322\n",
      "Gradient of b: -0.0017872995\n",
      "Loss: 0.000341, LR: 0.010000\n",
      "a: 0.6445719599723816, b: 0.9551847577095032\n",
      "----------------------------------------------\n",
      "Batch # 8\n",
      "----------------------------------------------\n",
      "Loss: 0.00020034437\n",
      "Gradient of a: 0.003239056\n",
      "Gradient of b: -0.0016984283\n",
      "Loss: 0.000200, LR: 0.010000\n",
      "a: 0.6366736888885498, b: 0.9628598093986511\n",
      "----------------------------------------------\n",
      "Batch # 9\n",
      "----------------------------------------------\n",
      "Loss: 5.9144917e-05\n",
      "Gradient of a: -9.5175805e-05\n",
      "Gradient of b: -0.0002181491\n",
      "Loss: 0.000059, LR: 0.010000\n",
      "a: 0.6296365261077881, b: 0.9697591662406921\n",
      "----------------------------------------------\n",
      "Batch # 10\n",
      "----------------------------------------------\n",
      "Loss: 0.00018719959\n",
      "Gradient of a: -0.0005291751\n",
      "Gradient of b: 0.0005946137\n",
      "Loss: 0.000187, LR: 0.010000\n",
      "a: 0.6233961582183838, b: 0.9758043885231018\n",
      "----------------------------------------------\n",
      "Batch # 11\n",
      "----------------------------------------------\n",
      "Loss: 0.00013641728\n",
      "Gradient of a: -0.0023962816\n",
      "Gradient of b: 0.0010936938\n",
      "Loss: 0.000136, LR: 0.010000\n",
      "a: 0.61809241771698, b: 0.9809839725494385\n",
      "----------------------------------------------\n",
      "Batch # 12\n",
      "----------------------------------------------\n",
      "Loss: 0.00010269486\n",
      "Gradient of a: 0.00011647009\n",
      "Gradient of b: -0.0006699516\n",
      "Loss: 0.000103, LR: 0.010000\n",
      "a: 0.6132966876029968, b: 0.985803484916687\n",
      "----------------------------------------------\n",
      "Batch # 13\n",
      "----------------------------------------------\n",
      "Loss: 0.00014905662\n",
      "Gradient of a: -0.0035443215\n",
      "Gradient of b: 0.0014674999\n",
      "Loss: 0.000149, LR: 0.002000\n",
      "a: 0.6094212532043457, b: 0.9898126721382141\n",
      "----------------------------------------------\n",
      "Batch # 14\n",
      "----------------------------------------------\n",
      "Loss: 0.00022243855\n",
      "Gradient of a: 0.0009674242\n",
      "Gradient of b: -0.0010121345\n",
      "Loss: 0.000222, LR: 0.002000\n",
      "a: 0.608695924282074, b: 0.9905838370323181\n",
      "----------------------------------------------\n",
      "Batch # 15\n",
      "----------------------------------------------\n",
      "Loss: 0.00023286657\n",
      "Gradient of a: -0.003187411\n",
      "Gradient of b: 0.0008533821\n",
      "Loss: 0.000233, LR: 0.002000\n",
      "a: 0.6081204414367676, b: 0.9912428259849548\n",
      "----------------------------------------------\n",
      "Batch # 16\n",
      "----------------------------------------------\n",
      "Loss: 0.00018239889\n",
      "Gradient of a: -0.0014172451\n",
      "Gradient of b: 0.0005366557\n",
      "Loss: 0.000182, LR: 0.002000\n",
      "a: 0.6076343655586243, b: 0.9918156266212463\n",
      "----------------------------------------------\n",
      "Batch # 17\n",
      "----------------------------------------------\n",
      "Loss: 0.0003634027\n",
      "Gradient of a: -0.007452595\n",
      "Gradient of b: 0.0030700534\n",
      "Loss: 0.000363, LR: 0.000400\n",
      "a: 0.6073868274688721, b: 0.9921881556510925\n",
      "----------------------------------------------\n",
      "Batch # 18\n",
      "----------------------------------------------\n",
      "Loss: 0.00021531593\n",
      "Gradient of a: 0.0036669464\n",
      "Gradient of b: -0.0018613191\n",
      "Loss: 0.000215, LR: 0.000400\n",
      "a: 0.6073235869407654, b: 0.9922729730606079\n",
      "----------------------------------------------\n",
      "Batch # 19\n",
      "----------------------------------------------\n",
      "Loss: 0.00027140958\n",
      "Gradient of a: -0.0076661417\n",
      "Gradient of b: 0.0034525038\n",
      "Loss: 0.000271, LR: 0.000400\n",
      "a: 0.6073053479194641, b: 0.9923170208930969\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████▌                               | 1/2 [42:41<42:41, 2561.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 0\n",
      "----------------------------------------------\n",
      "Loss: 0.00020098765\n",
      "Gradient of a: -0.0043144403\n",
      "Gradient of b: 0.0015344885\n",
      "Loss: 0.000201, LR: 0.000400\n",
      "a: 0.60731041431427, b: 0.9923425912857056\n",
      "----------------------------------------------\n",
      "Batch # 1\n",
      "----------------------------------------------\n",
      "Loss: 0.00033154\n",
      "Gradient of a: 0.0013025022\n",
      "Gradient of b: 0.00033376197\n",
      "Loss: 0.000332, LR: 0.000080\n",
      "a: 0.6073084473609924, b: 0.9923626780509949\n",
      "----------------------------------------------\n",
      "Batch # 2\n",
      "----------------------------------------------\n",
      "Loss: 0.00045103848\n",
      "Gradient of a: -0.008300493\n",
      "Gradient of b: 0.0038167166\n",
      "Loss: 0.000451, LR: 0.000080\n",
      "a: 0.6073164343833923, b: 0.9923590421676636\n",
      "----------------------------------------------\n",
      "Batch # 3\n",
      "----------------------------------------------\n",
      "Loss: 0.00019651694\n",
      "Gradient of a: -0.0029143838\n",
      "Gradient of b: 0.0014634749\n",
      "Loss: 0.000197, LR: 0.000080\n",
      "a: 0.6073266267776489, b: 0.9923529624938965\n",
      "----------------------------------------------\n",
      "Batch # 4\n",
      "----------------------------------------------\n",
      "Loss: 0.000279769\n"
     ]
    }
   ],
   "source": [
    "# Ensure defualt printing options\n",
    "torch.set_printoptions(profile=\"default\") # reset\n",
    "# Create a training instance\n",
    "ARRG = ARRG(epochs = epochs, dim_multiplicity = dim_multiplicity, dim_accept_reject = dim_accept_reject, over_sample_factor = over_sample_factor,\n",
    "\t\t\t\t\t\t   params_base = params_base, sim_observable_dataloader = sim_observable_dataloader, sim_kinematics_z_dataloader = sim_accept_reject_dataloader, \n",
    "\t\t\t\t\t\t   sim_kinematics_mT_dataloader = sim_mT_dataloader, exp_observable_dataloader = exp_observable_dataloader, print_details = print_details, \n",
    "\t\t\t\t\t\t   results_dir = results_dir, params_init = params_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer\n",
    "optimizer = torch.optim.Adam(ARRG.weight_nexus.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(macroscopic_trainer.weight_nexus.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode= 'min', factor = 0.2, patience = 3)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.1)\n",
    "#scheduler = None\n",
    "\n",
    "# Train!\n",
    "a_b_final, a_b_search = ARRG.train_ARRG(optimizer = optimizer, scheduler = scheduler)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(\"Final parameters: a =\", a_b_final[0], 'b =', a_b_final[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
