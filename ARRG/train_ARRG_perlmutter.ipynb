{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ARRG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservableDataset(Dataset):\n",
    "\t\"\"\"\n",
    "\tConverts observable dataset into PyTorch syntax.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, data):\n",
    "\t\tself.data = data\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.data.shape[0]\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tsample = self.data[idx]\n",
    "\t\treturn sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do some data proccessing to extract the event-level observables we want to train with. Remember that the event level data-structure contains arrays of hadron-level kinematic data $p_x, p_y, p_z, E, m_h$. To need to compute event-level, experiment-level, or macroscopic-level observables that can actually be observed from experiment.\n",
    "\n",
    "I'll only use hadron multiplicity but we could also generate more distributions (sphericity, thrust, shape parameters, energy correlators, etc.) if desired. Hadron multiplicity is extremely sensitive to $a$ and $b$ so we can tune these parameters essentially exclusively on hadron multiplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental observable shape: (1000000, 50, 5)\n",
      "Simulated observable shape: (1000000, 50, 5)\n",
      "Simulated z shape: (1000000, 50, 100)\n",
      "Experimental multiplicity shape: torch.Size([100000])\n",
      "Simulated multiplicity shape: torch.Size([100000])\n",
      "Simulated z shape: torch.Size([100000, 50, 100])\n",
      "Simulated mT shape: torch.Size([100000, 50])\n"
     ]
    }
   ],
   "source": [
    "# Paths to the datasets on perlmutter\n",
    "exp_hadrons_PATH = '/global/cfs/projectdirs/m3246/hadronization-tune-mlrw-data/pgun_qqbar_hadrons_a_0.68_b_0.98_sigma_0.335_N_1e6.npy'\n",
    "#exp_accept_reject_PATH = '/global/cfs/projectdirs/m3246/hadronization-tune-mlrw-data/pgun_qqbar_accept_reject_z_a_0.68_b_0.98_sigma_0.335_N_1e6.npy'\n",
    "sim_hadrons_PATH = '/global/cfs/projectdirs/m3246/hadronization-tune-mlrw-data/pgun_qqbar_hadrons_a_0.72_b_0.88_sigma_0.335_N_1e6.npy'\n",
    "sim_accept_reject_PATH = '/global/cfs/projectdirs/m3246/hadronization-tune-mlrw-data/pgun_qqbar_accept_reject_z_a_0.72_b_0.88_sigma_0.335_N_1e6.npy'\n",
    "\n",
    "# Load the arrays\n",
    "exp_hadrons       = np.load(exp_hadrons_PATH, mmap_mode=\"r\")\n",
    "sim_hadrons       = np.load(sim_hadrons_PATH, mmap_mode=\"r\")\n",
    "sim_accept_reject = np.load(sim_accept_reject_PATH, mmap_mode = \"r\")\n",
    "\n",
    "# Print dataset shapes\n",
    "print('Experimental observable shape:', exp_hadrons.shape)\n",
    "print('Simulated observable shape:', sim_hadrons.shape)\n",
    "print('Simulated z shape:', sim_accept_reject.shape)\n",
    "\n",
    "# Restrict to a subset of the full dataset (for memory)\n",
    "N_events = int(100000)\n",
    "\n",
    "# Extract the hadron multiplicity\n",
    "exp_mult = np.array([len(exp_hadrons[i,:][np.abs(exp_hadrons[i,:,0]) > 0.0]) for i in range(N_events)])\n",
    "sim_mult = np.array([len(sim_hadrons[i,:][np.abs(sim_hadrons[i,:,0]) > 0.0]) for i in range(N_events)])\n",
    "# Extract the transverse mass\n",
    "sim_mT   = np.sqrt(sim_hadrons[:,:,0]**2 + sim_hadrons[:,:,1]**2 + sim_hadrons[:,:,4]**2)\n",
    "\n",
    "# Convert into torch objects\n",
    "sim_mult          = torch.Tensor(sim_mult[0:N_events].copy())\n",
    "sim_accept_reject = torch.Tensor(sim_accept_reject[0:N_events].copy())\n",
    "sim_mT            = torch.Tensor(sim_mT[0:N_events].copy())\n",
    "exp_mult          = torch.Tensor(exp_mult[0:N_events].copy())\n",
    "\n",
    "# Check the accepted z-values, if z == 1 reduce it by epsilon (a very nasty bug to find).\n",
    "# The a-coefficient when computing the likelihood has a term propotional to log(1-z). If \n",
    "# z = 1, this term diverges to -inf and completely destroys the backward pass.\n",
    "epsilon = 1e-5\n",
    "sim_accept_reject[sim_accept_reject == 1] = 1 - epsilon\n",
    "\n",
    "# Print dataset shapes\n",
    "print('Experimental multiplicity shape:', exp_mult.shape)\n",
    "print('Simulated multiplicity shape:', sim_mult.shape)\n",
    "print('Simulated z shape:', sim_accept_reject.shape)\n",
    "print('Simulated mT shape:', sim_mT.shape)\n",
    "\n",
    "# Prepare data for DataLoader\n",
    "sim_mult          = ObservableDataset(sim_mult)\n",
    "sim_accept_reject = ObservableDataset(sim_accept_reject)\n",
    "sim_mT            = ObservableDataset(sim_mT)\n",
    "exp_mult          = ObservableDataset(exp_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size -- TBD: Implement batch size scheduler\n",
    "batch_size = 5000\n",
    "\n",
    "# Initialize data-loaders\n",
    "sim_observable_dataloader    = DataLoader(sim_mult, batch_size = batch_size, shuffle = False)\n",
    "sim_accept_reject_dataloader = DataLoader(sim_accept_reject, batch_size = batch_size, shuffle = False)\n",
    "sim_mT_dataloader            = DataLoader(sim_mT, batch_size = batch_size, shuffle = False)\n",
    "exp_observable_dataloader    = DataLoader(exp_mult, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each event has been zero-padded to a length of 50\n",
      "Each emission has been zero-padded to a length of 100\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 2\n",
    "over_sample_factor = 10.0\n",
    "learning_rate = 0.01\n",
    "fixed_binning = True\n",
    "# Length of event buffer\n",
    "dim_multiplicity = sim_accept_reject_dataloader.dataset.data.shape[1]\n",
    "dim_accept_reject = sim_accept_reject_dataloader.dataset.data.shape[2]\n",
    "\n",
    "print('Each event has been zero-padded to a length of', dim_multiplicity)\n",
    "print('Each emission has been zero-padded to a length of', dim_accept_reject)\n",
    "\n",
    "# Define base parameters of simulated data (a, b)\n",
    "params_base = torch.tensor([0.72, 0.88])\n",
    "# If params_init is set equal to None, the tuned parameters are initialized to the base parameters\n",
    "params_init = None\n",
    "#params_init = torch.tensor([0.6, 1.5])\n",
    "\n",
    "print_details = True\n",
    "results_dir = r'./ARRG_a_b_tune_perlmutter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure defualt printing options\n",
    "torch.set_printoptions(profile=\"default\") # reset\n",
    "# Create a training instance\n",
    "ARRG = ARRG(epochs = epochs, dim_multiplicity = dim_multiplicity, dim_accept_reject = dim_accept_reject, over_sample_factor = over_sample_factor,\n",
    "\t\t\t\t\t\t   params_base = params_base, sim_observable_dataloader = sim_observable_dataloader, sim_kinematics_z_dataloader = sim_accept_reject_dataloader, \n",
    "\t\t\t\t\t\t   sim_kinematics_mT_dataloader = sim_mT_dataloader, exp_observable_dataloader = exp_observable_dataloader, print_details = print_details, \n",
    "\t\t\t\t\t\t   results_dir = results_dir, params_init = params_init, fixed_binning = fixed_binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 0\n",
      "----------------------------------------------\n",
      "Loss: 276.6980214280278\n",
      "Gradient of a: 2422.0579\n",
      "Gradient of b: -1092.8967\n",
      "Loss: 276.698021, LR: 0.010000\n",
      "a: 0.7100000381469727, b: 0.8899999856948853\n",
      "----------------------------------------------\n",
      "Batch # 1\n",
      "----------------------------------------------\n",
      "Loss: 605.9722242535802\n",
      "Gradient of a: 2659.8076\n",
      "Gradient of b: -1326.4382\n",
      "Loss: 605.972224, LR: 0.010000\n",
      "a: 0.6999865770339966, b: 0.9000037908554077\n",
      "----------------------------------------------\n",
      "Batch # 2\n",
      "----------------------------------------------\n",
      "Loss: 229.6267708911248\n",
      "Gradient of a: 1870.1982\n",
      "Gradient of b: -1074.7853\n",
      "Loss: 229.626771, LR: 0.010000\n",
      "a: 0.6901715397834778, b: 0.9099478125572205\n",
      "----------------------------------------------\n",
      "Batch # 3\n",
      "----------------------------------------------\n",
      "Loss: 134.85062724841129\n",
      "Gradient of a: 1774.5228\n",
      "Gradient of b: -922.95856\n",
      "Loss: 134.850627, LR: 0.010000\n",
      "a: 0.6804753541946411, b: 0.9197698831558228\n",
      "----------------------------------------------\n",
      "Batch # 4\n",
      "----------------------------------------------\n",
      "Loss: 150.58815105499886\n",
      "Gradient of a: 1170.0272\n",
      "Gradient of b: -785.8914\n",
      "Loss: 150.588151, LR: 0.010000\n",
      "a: 0.6711567640304565, b: 0.9294148087501526\n",
      "----------------------------------------------\n",
      "Batch # 5\n",
      "----------------------------------------------\n",
      "Loss: 82.25383362048717\n",
      "Gradient of a: 1274.7406\n",
      "Gradient of b: -659.00586\n",
      "Loss: 82.253834, LR: 0.010000\n",
      "a: 0.6620177626609802, b: 0.9388260841369629\n",
      "----------------------------------------------\n",
      "Batch # 6\n",
      "----------------------------------------------\n",
      "Loss: 431.9948804036944\n",
      "Gradient of a: 660.9881\n",
      "Gradient of b: -513.31006\n",
      "Loss: 431.994880, LR: 0.010000\n",
      "a: 0.6534139513969421, b: 0.9479094743728638\n",
      "----------------------------------------------\n",
      "Batch # 7\n",
      "----------------------------------------------\n",
      "Loss: 107.12657539219103\n",
      "Gradient of a: 719.24567\n",
      "Gradient of b: -346.4824\n",
      "Loss: 107.126575, LR: 0.010000\n",
      "a: 0.6451765298843384, b: 0.9565219879150391\n",
      "----------------------------------------------\n",
      "Batch # 8\n",
      "----------------------------------------------\n",
      "Loss: 99.98094775219292\n",
      "Gradient of a: 177.29716\n",
      "Gradient of b: -128.7018\n",
      "Loss: 99.980948, LR: 0.010000\n",
      "a: 0.6376897096633911, b: 0.9644068479537964\n",
      "----------------------------------------------\n",
      "Batch # 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         | 0/2 [22:49<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.1)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#scheduler = None\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train!\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m a_b_final, a_b_search \u001b[38;5;241m=\u001b[39m \u001b[43mARRG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ARRG\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal parameters: a =\u001b[39m\u001b[38;5;124m\"\u001b[39m, a_b_final[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb =\u001b[39m\u001b[38;5;124m'\u001b[39m, a_b_final[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/global/u1/t/tmenzo/hadronization-tune-mlrw/ARRG/ARRG.py:95\u001b[0m, in \u001b[0;36mARRG.train_ARRG\u001b[0;34m(self, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     93\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Compute the weights\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_nexus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpseudo_chi2_loss(z, w, weights) \u001b[38;5;66;03m#/ x.shape[0]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/global/u1/t/tmenzo/hadronization-tune-mlrw/ARRG/lund_weight.py:93\u001b[0m, in \u001b[0;36mLundWeight.forward\u001b[0;34m(self, z, mT, observable)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(event_mult):\n\u001b[1;32m     92\u001b[0m     reject_values \u001b[38;5;241m=\u001b[39m reject_tensor[j, reject_tensor[j, :] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.\u001b[39m]\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mreject_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     94\u001b[0m         reject_weights_i \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mover_sample_factor \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(reject_values, mT[i, j], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_a, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_b)) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mover_sample_factor \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(reject_values, mT[i, j], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_base[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_base[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     95\u001b[0m         reject_weights[j] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mprod(reject_weights_i)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the optimizer\n",
    "optimizer = torch.optim.Adam(ARRG.weight_nexus.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(macroscopic_trainer.weight_nexus.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode= 'min', factor = 0.2, patience = 3)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.1)\n",
    "#scheduler = None\n",
    "\n",
    "# Train!\n",
    "a_b_final, a_b_search = ARRG.train_ARRG(optimizer = optimizer, scheduler = scheduler)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(\"Final parameters: a =\", a_b_final[0], 'b =', a_b_final[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
